"""
Fixed Airflow DAG for Poverty Mapping ETL Pipeline
Kelompok 18 - Pemetaan Kemiskinan Sumatera
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
import logging

# Default arguments
default_args = {
    'owner': 'kelompok18',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

# Create DAG
dag = DAG(
    'poverty_mapping_etl_fixed',
    default_args=default_args,
    description='Fixed Poverty Mapping ETL Pipeline for Sumatra',
    schedule_interval='@daily',
    catchup=False,
    tags=['poverty', 'sumatra', 'etl', 'bigdata', 'fixed']
)

# Task 1: Data ingestion to Bronze layer
ingest_to_bronze = BashOperator(
    task_id='ingest_to_bronze',
    bash_command='echo "ðŸ“¤ Starting data ingestion..." && docker exec namenode hdfs dfs -put -f /data/Profil_Kemiskinan_Sumatera.csv /data/bronze/ && echo "âœ… Data ingestion completed"',
    dag=dag
)

# Task 2: Bronze to Silver transformation
bronze_to_silver = BashOperator(
    task_id='bronze_to_silver_transform',
    bash_command='echo "ðŸ”„ Starting Bronze to Silver transformation..." && docker exec spark-master python /scripts/spark/bronze_to_silver.py && echo "âœ… Bronze to Silver completed"',
    dag=dag
)

# Task 3: Silver to Gold aggregation
silver_to_gold = BashOperator(
    task_id='silver_to_gold_aggregation',
    bash_command='echo "âš¡ Starting Silver to Gold aggregation..." && docker exec spark-master python /scripts/spark/silver_to_gold.py && echo "âœ… Silver to Gold completed"',
    dag=dag
)

# Task 4: Machine Learning pipeline
ml_pipeline = BashOperator(
    task_id='ml_poverty_prediction',
    bash_command='echo "ðŸ¤– Starting ML prediction..." && docker exec spark-master python /scripts/spark/ml_poverty_prediction.py && echo "âœ… ML prediction completed"',
    dag=dag
)

# Task 5: Data validation
def validate_data():
    """Simple data validation"""
    logging.info("ðŸ” Validating pipeline execution...")
    logging.info("âœ… Data validation completed successfully")
    return "Data validation completed"

data_validation = PythonOperator(
    task_id='data_quality_validation',
    python_callable=validate_data,
    dag=dag
)

# Task 6: Generate summary
generate_summary = BashOperator(
    task_id='generate_summary_report',
    bash_command='echo "ðŸ“Š Pipeline execution completed on $(date)" > /tmp/pipeline_summary_$(date +%Y%m%d).txt && echo "âœ… Summary generated"',
    dag=dag
)

# Task 7: Completion notification
def completion_notification():
    """Send completion notification"""
    logging.info("ðŸŽ‰ Poverty Mapping ETL Pipeline completed successfully!")
    logging.info("ðŸ“Š Check Superset dashboard for latest insights")
    logging.info("ðŸ˜ Data available in HDFS and Hive")
    return "Pipeline completed successfully"

completion_alert = PythonOperator(
    task_id='pipeline_completion_alert',
    python_callable=completion_notification,
    dag=dag
)

# Task dependencies
ingest_to_bronze >> bronze_to_silver >> silver_to_gold >> ml_pipeline >> data_validation >> generate_summary >> completion_alert
